#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
"""

import os
import shutil
import json
import asyncio
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import tempfile
import logging
from concurrent.futures import ThreadPoolExecutor
import time
import ast
import re
from collections import defaultdict

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TranslationConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
    source_project_path: str
    target_language: str = "English"
    source_language: str = "Russian"
    chunk_size: int = 150
    preserve_patterns: List[str] = None  # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã)
    custom_instructions: str = ""
    supported_extensions: List[str] = None
    exclude_dirs: List[str] = None
    exclude_files: List[str] = None
    llm_provider: str = "groq"  # groq, openai, anthropic
    api_key: str = ""
    model_name: str = "openai/gpt-oss-120b"
    max_concurrent_requests: int = 10
    
    def __post_init__(self):
        if self.preserve_patterns is None:
            self.preserve_patterns = [
                r'[\u4e00-\u9fff]',  # –ö–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
                r'[\u3040-\u309f]',  # –Ø–ø–æ–Ω—Å–∫–∞—è —Ö–∏—Ä–∞–≥–∞–Ω–∞
                r'[\u30a0-\u30ff]',  # –Ø–ø–æ–Ω—Å–∫–∞—è –∫–∞—Ç–∞–∫–∞–Ω–∞
            ]
        
        if self.supported_extensions is None:
            self.supported_extensions = [
                '.py', '.js', '.ts', '.jsx', '.tsx', '.html', '.css', '.scss',
                '.java', '.cpp', '.c', '.h', '.cs', '.php', '.rb', '.go',
                '.rs', '.swift', '.kt', '.dart', '.vue', '.svelte'
            ]
        
        if self.exclude_dirs is None:
            self.exclude_dirs = [
                'node_modules', '__pycache__', '.git', '.svn', '.hg',
                'venv', 'env', '.env', 'dist', 'build', '.next',
                'target', 'bin', 'obj', '.pytest_cache', '.mypy_cache'
            ]
        
        if self.exclude_files is None:
            self.exclude_files = [
                'README.md', 'LICENSE', 'CHANGELOG.md', 'requirements.txt',
                'package.json', 'package-lock.json', 'yarn.lock',
                '.gitignore', '.dockerignore', 'Dockerfile'
            ]


class ProjectAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self, config: TranslationConfig):
        self.config = config
        
    def analyze_project(self, project_path: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞"""
        project_info = {
            'total_files': 0,
            'translatable_files': 0,
            'file_types': {},
            'directory_structure': {},
            'estimated_chunks': 0,
            'files_to_translate': []
        }
        
        for root, dirs, files in os.walk(project_path):
            # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [d for d in dirs if d not in self.config.exclude_dirs]
            
            for file in files:
                project_info['total_files'] += 1
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, project_path)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                _, ext = os.path.splitext(file)
                if ext in self.config.supported_extensions and file not in self.config.exclude_files:
                    project_info['translatable_files'] += 1
                    project_info['file_types'][ext] = project_info['file_types'].get(ext, 0) + 1
                    
                    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —á–∞–Ω–∫–æ–≤
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            lines = sum(1 for _ in f)
                            chunks = (lines // self.config.chunk_size) + 1
                            project_info['estimated_chunks'] += chunks
                            
                            project_info['files_to_translate'].append({
                                'path': relative_path,
                                'full_path': file_path,
                                'extension': ext,
                                'lines': lines,
                                'estimated_chunks': chunks
                            })
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
        
        return project_info


class FileChunker:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
    
    def __init__(self, config: TranslationConfig):
        self.config = config
        
    def split_file(self, file_path: str, output_dir: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –Ω–∞ —á–∞–Ω–∫–∏ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –º–∞—Ä–∫–µ—Ä–æ–≤ –≥—Ä–∞–Ω–∏—Ü"""
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            for i in range(0, len(lines), self.config.chunk_size):
                chunk_lines = lines[i:i + self.config.chunk_size]
                chunk_index = i // self.config.chunk_size
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –Ω–∞—á–∞–ª–∞ —á–∞–Ω–∫–∞ (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ)
                if i > 0:
                    chunk_content = f"---CHUNK_START_{chunk_index:04d}---\n" + ''.join(chunk_lines)
                else:
                    chunk_content = ''.join(chunk_lines)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –æ–∫–æ–Ω—á–∞–Ω–∏—è —á–∞–Ω–∫–∞ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
                if i + self.config.chunk_size < len(lines):
                    chunk_content += f"---CHUNK_END_{chunk_index:04d}---\n"
                
                # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —á–∞–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑—É—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –ø—Ä–æ–µ–∫—Ç–∞
                relative_path = os.path.relpath(file_path, self.config.source_project_path)
                safe_name = relative_path.replace(os.sep, '_').replace('.', '_')
                chunk_filename = f"{safe_name}_chunk_{chunk_index:04d}.txt"
                chunk_path = os.path.join(output_dir, chunk_filename)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫ —Å –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                chunk_data = {
                    'original_file': file_path,
                    'chunk_index': chunk_index,
                    'start_line': i,
                    'end_line': min(i + self.config.chunk_size, len(lines)),
                    'content': chunk_content,
                    'has_start_marker': i > 0,
                    'has_end_marker': i + self.config.chunk_size < len(lines)
                }
                
                with open(chunk_path, 'w', encoding='utf-8') as chunk_file:
                    json.dump(chunk_data, chunk_file, ensure_ascii=False, indent=2)
                
                chunks.append(chunk_path)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            
        return chunks


class LLMTranslator:
    """–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self, config: TranslationConfig):
        self.config = config
        self.session = None
        
    async def setup_client(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        if self.config.llm_provider == "groq":
            try:
                from groq import AsyncGroq
                self.client = AsyncGroq(api_key=self.config.api_key)
            except ImportError:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ groq: pip install groq")
                
        elif self.config.llm_provider == "openai":
            try:
                from openai import AsyncOpenAI
                self.client = AsyncOpenAI(api_key=self.config.api_key)
            except ImportError:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai: pip install openai")
                
        elif self.config.llm_provider == "anthropic":
            try:
                import anthropic
                self.client = anthropic.AsyncAnthropic(api_key=self.config.api_key)
            except ImportError:
                raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ anthropic: pip install anthropic")
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.config.llm_provider}")
    
    def create_system_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏"""
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–Ω–µ–∏–∑–º–µ–Ω–Ω—ã–µ) - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø v2.2
        system_instructions = """
=== ULTRA-STRICT CODE TRANSLATION PROTOCOL v2.3 ===
You are a PRECISION code translator with ZERO tolerance for deviations.
Mission: Translate {source_lang} natural language to {target_lang} with ABSOLUTE preservation of code structure.

=== ABSOLUTE REQUIREMENTS (VIOLATION = COMPLETE FAILURE) ===
1. PRESERVE LINE STRUCTURE: Maintain similar line count (minor deviations acceptable for readability)
2. EXACT PRESERVATION: indentation, whitespace, ALL brackets (), [], {{}}, semicolons, commas
3. CAREFUL PRESERVATION: blank lines, empty lines, line breaks (maintain logical spacing)
4. EXACT PRESERVATION: special characters, operators, punctuation marks
5. EXACT PRESERVATION: ALL triple quotes \"\"\" and ''' for docstrings - NEVER REMOVE THEM
6. OUTPUT RESTRICTION: ONLY translated code - ZERO additional content whatsoever
7. ABSOLUTELY FORBIDDEN: any explanatory text, meta-comments, process descriptions

=== TRANSLATION SCOPE - TRANSLATE ONLY THESE ===
‚Ä¢ User-facing comments: # comment, // comment, /* comment */, <!-- comment -->
‚Ä¢ Documentation strings: \"\"\"docstring\"\"\", '''docstring''', /** docstring */
‚Ä¢ User interface text strings with natural language content
‚Ä¢ Error/log/notification messages meant for end users
‚Ä¢ Descriptive string literals (NOT technical identifiers or constants)

=== ABSOLUTE TRANSLATION EXCLUSIONS - NEVER TRANSLATE ===
‚Ä¢ ALL programming syntax: keywords, operators, control structures  
‚Ä¢ ALL identifiers: variables, functions, classes, methods, modules
‚Ä¢ ALL technical constants: paths, URLs, endpoints, database queries
‚Ä¢ Import statements, module references, package names
‚Ä¢ Regular expressions, SQL, JSON keys, XML attributes, configuration
‚Ä¢ Version numbers, timestamps, hashes, UUIDs, technical IDs
‚Ä¢ Mathematical formulas, algorithms, data structures
‚Ä¢ Chunk boundary markers: ---CHUNK_START_XXXX---, ---CHUNK_END_XXXX---
‚Ä¢ Programming language constructs and patterns
‚Ä¢ Args:, Returns:, Raises:, Parameters:, Note:, Warning: in docstrings
‚Ä¢ String constants used by program logic ("strong", "weak", "active", etc.)
‚Ä¢ Enumeration values, status codes, technical classifications

=== SPECIAL CHARACTER PRESERVATION ===
‚Ä¢ Preserve ALL characters matching patterns specified in PROJECT-SPECIFIC INSTRUCTIONS
‚Ä¢ When uncertain about special characters ‚Üí PRESERVE exactly as provided
‚Ä¢ Mathematical symbols [\u2200-\u22FF], Technical symbols [\u2300-\u23FF]
‚Ä¢ Any CJK characters [\u4E00-\u9FFF], [\u3040-\u309F], [\u30A0-\u30FF], [\uAC00-\uD7AF]
‚Ä¢ Follow project-specific preservation rules from custom instructions

=== META-COMMENT BAN - ABSOLUTELY FORBIDDEN ===
üö´ NEVER add ANY of these phrases:
‚Ä¢ "No changes needed"
‚Ä¢ "Already in [language]"
‚Ä¢ "Only [something] was translated"
‚Ä¢ "Most text is already"
‚Ä¢ "Translation complete"
‚Ä¢ "Code remains the same"
‚Ä¢ ANY explanatory comments about the translation process
‚Ä¢ ANY meta-commentary whatsoever

=== FLEXIBLE LINE COUNT CONTROL ===
üìè INPUT has N lines ‚Üí OUTPUT should have SIMILAR line count (minor variations acceptable):
‚Ä¢ Maintain logical structure and readability
‚Ä¢ Avoid excessive line additions or removals
‚Ä¢ Preserve meaningful empty lines and spacing
‚Ä¢ Minor adjustments for readability are acceptable
‚Ä¢ Focus on preserving code structure over exact line matching

=== FORMATTING PROTOCOL - ZERO TOLERANCE ===
‚Ä¢ FORBIDDEN: markdown blocks (```), HTML tags, any markup
‚Ä¢ FORBIDDEN: "Translation:", "Result:", "Output:", explanations
‚Ä¢ FORBIDDEN: process commentary, status updates, notes
‚Ä¢ REQUIRED: identical visual structure and formatting
‚Ä¢ REQUIRED: preserve tabs vs spaces exactly as original
‚Ä¢ REQUIRED: same line endings, character encoding

=== PRE-OUTPUT VERIFICATION CHECKLIST ===
Before returning result, VERIFY:
‚úÖ Line count: input lines = output lines (count manually)
‚úÖ Structure: all (), [], {{}} brackets match exactly  
‚úÖ Whitespace: indentation, spacing identical
‚úÖ Special chars: follow project-specific preservation rules
‚úÖ No meta-comments: zero explanatory text added
‚úÖ Only user comments/strings translated
‚úÖ Technical terms, constants, identifiers unchanged
‚úÖ Code logic and flow completely preserved

=== ERROR HANDLING ===
‚Ä¢ Uncertain about translation? ‚Üí PRESERVE original
‚Ä¢ Line count might change? ‚Üí PRESERVE original
‚Ä¢ Structure might break? ‚Üí PRESERVE original
‚Ä¢ Special characters detected? ‚Üí Follow project rules or PRESERVE exactly
‚Ä¢ Technical term unclear? ‚Üí PRESERVE unchanged

=== FINAL OUTPUT REQUIREMENTS ===
Return EXCLUSIVELY:
‚Ä¢ The translated code with same structure
‚Ä¢ Same encoding, line endings as input
‚Ä¢ Zero additional content
‚Ä¢ Zero metadata or comments
‚Ä¢ Zero explanatory text

üî• CRITICAL: Any violation of these rules = COMPLETE FAILURE
üíÄ Adding meta-comments = IMMEDIATE FAILURE
‚ö° Wrong line count = IMMEDIATE FAILURE
üö® Ignoring project-specific preservation rules = IMMEDIATE FAILURE
        """
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.config.custom_instructions:
            system_instructions += "\n\n=== PROJECT-SPECIFIC INSTRUCTIONS ===\n" + self.config.custom_instructions
        
        return system_instructions

    def create_user_prompt(self, content: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç"""
        line_count = len(content.splitlines())
        return f"""INPUT ({line_count} lines):
{content}

OUTPUT ({line_count} lines required - translate {self.config.source_language} to {self.config.target_language}):"""

    async def translate_chunk(self, chunk_content: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ –∫–æ–¥–∞"""
        try:
            if self.config.llm_provider == "groq":
                response = await self.client.chat.completions.create(
                    model=self.config.model_name,
                    messages=[
                        {"role": "system", "content": self.create_system_prompt()},
                        {"role": "user", "content": self.create_user_prompt(chunk_content)}
                    ],
                    temperature=0.0,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (—Å–Ω–∏–∂–µ–Ω–æ)
                    max_tokens=4096
                )
                return response.choices[0].message.content.strip()
                
            elif self.config.llm_provider == "openai":
                response = await self.client.chat.completions.create(
                    model=self.config.model_name,
                    messages=[
                        {"role": "system", "content": self.create_system_prompt()},
                        {"role": "user", "content": self.create_user_prompt(chunk_content)}
                    ],
                    temperature=0.0,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (—Å–Ω–∏–∂–µ–Ω–æ)
                    max_tokens=4096
                )
                return response.choices[0].message.content.strip()
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞–Ω–∫–∞: {e}")
            return chunk_content  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ


class ChunkMerger:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ñ–∞–π–ª—ã"""
    
    def merge_chunks(self, chunks_dir: str, output_file: str, source_project_path: str) -> bool:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Ä–∫–µ—Ä–æ–≤ –≥—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å —Ñ–∞–π–ª–∞ –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
            project_root = os.path.basename(source_project_path)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∏–∑ output_file
            if '_translated' in output_file:
                rel_output_path = output_file.split('_translated' + os.sep, 1)[1] if os.sep + '_translated' + os.sep in output_file else output_file.split('_translated/', 1)[1]
            else:
                rel_output_path = os.path.basename(output_file)
            
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–∞–Ω–∫–æ–≤ (–∫–∞–∫ –≤ split_file)
            safe_name = rel_output_path.replace(os.sep, '_').replace('.', '_')
            
            # –ò—â–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —Å —ç—Ç–∏–º –∏–º–µ–Ω–µ–º
            chunk_files = []
            for chunk_file in os.listdir(chunks_dir):
                if chunk_file.startswith(safe_name + '_chunk_') and chunk_file.endswith('.txt'):
                    chunk_path = os.path.join(chunks_dir, chunk_file)
                    try:
                        with open(chunk_path, 'r', encoding='utf-8') as f:
                            chunk_data = json.load(f)
                        chunk_files.append((chunk_path, chunk_data['chunk_index'], chunk_data))
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —á–∞–Ω–∫ {chunk_path}: {e}")
                        
            if not chunk_files:
                logger.error(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —á–∞–Ω–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {output_file}, –∏—Å–∫–∞–ª–∏ –ø–æ –∏–º–µ–Ω–∏: {safe_name}")
                return False
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É —á–∞–Ω–∫–∞
            chunk_files.sort(key=lambda x: x[1])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Ä–∫–µ—Ä–æ–≤
            merged_lines = []
            
            for i, (chunk_path, chunk_index, chunk_data) in enumerate(chunk_files):
                chunk_content = chunk_data['content']
                
                # –£–¥–∞–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –≥—Ä–∞–Ω–∏—Ü, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è
                cleaned_content = self.remove_boundary_markers(chunk_content)
                
                # –î–ª—è –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∫ –µ—Å—Ç—å
                if i == 0:
                    merged_lines.append(cleaned_content)
                else:
                    # –î–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ —Ä–∞–∑—Ä—ã–≤–æ–≤
                    if merged_lines and merged_lines[-1] and not merged_lines[-1].endswith('\n'):
                        merged_lines[-1] += '\n'  # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑—Ä—ã–≤ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    merged_lines.append(cleaned_content)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            merged_content = ''.join(merged_lines)
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(merged_content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            original_file_path = chunk_files[0][2].get('original_file') if chunk_files else None
            if original_file_path and os.path.exists(original_file_path):
                try:
                    with open(original_file_path, 'r', encoding='utf-8') as f:
                        original_lines = f.readlines()
                    
                    merged_file_lines = merged_content.splitlines(keepends=True)
                    if len(original_lines) != len(merged_file_lines):
                        logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º: {len(original_lines)} -> {len(merged_file_lines)}")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
            
            logger.debug(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(chunk_files)} —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞ {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–ª—è {output_file}: {e}")
            return False
    
    def remove_boundary_markers(self, content: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –º–∞—Ä–∫–µ—Ä—ã –≥—Ä–∞–Ω–∏—Ü –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞–Ω–∫–∞"""
        import re
        
        # –£–¥–∞–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ —á–∞–Ω–∫–æ–≤ (–∫—É–¥–∞ –±—ã –æ–Ω–∏ –Ω–µ –ø–æ–ø–∞–ª–∏)
        content = re.sub(r'---CHUNK_START_\d+---\n?', '', content)
        content = re.sub(r'---CHUNK_END_\d+---\n?', '', content)
        
        return content


class ProjectTranslator:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self, config: TranslationConfig):
        self.config = config
        self.analyzer = ProjectAnalyzer(config)
        self.chunker = FileChunker(config)
        self.translator = LLMTranslator(config)
        self.merger = ChunkMerger()
        
    async def translate_project(self):
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –≤–µ—Å—å –ø—Ä–æ–µ–∫—Ç"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ –ø—Ä–æ–µ–∫—Ç–∞...")
        
        # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç
        logger.info("üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞...")
        project_info = self.analyzer.analyze_project(self.config.source_project_path)
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {project_info['total_files']}")
        logger.info(f"–§–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: {project_info['translatable_files']}")
        logger.info(f"–û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {project_info['estimated_chunks']}")
        
        # 2. –°–æ–∑–¥–∞–µ–º —Ä–∞–±–æ—á–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        with tempfile.TemporaryDirectory() as temp_dir:
            backup_dir = os.path.join(temp_dir, "backup")
            chunks_dir = os.path.join(temp_dir, "chunks")
            translated_chunks_dir = os.path.join(temp_dir, "translated_chunks")
            
            # 3. –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            logger.info("üíæ –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ø—Ä–æ–µ–∫—Ç–∞...")
            shutil.copytree(self.config.source_project_path, backup_dir)
            
            # 4. –†–∞–∑–±–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏
            logger.info("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏...")
            os.makedirs(chunks_dir, exist_ok=True)
            all_chunks = []
            
            for file_info in project_info['files_to_translate']:
                chunks = self.chunker.split_file(file_info['full_path'], chunks_dir)
                all_chunks.extend(chunks)
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
            
            # 5. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
            logger.info("ü§ñ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LLM –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞...")
            await self.translator.setup_client()
            
            # 6. –ü–µ—Ä–µ–≤–æ–¥–∏–º —á–∞–Ω–∫–∏
            logger.info("üåê –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ —á–∞–Ω–∫–æ–≤...")
            os.makedirs(translated_chunks_dir, exist_ok=True)
            
            semaphore = asyncio.Semaphore(self.config.max_concurrent_requests)
            tasks = []
            
            for chunk_path in all_chunks:
                task = self.translate_chunk_with_semaphore(semaphore, chunk_path, translated_chunks_dir)
                tasks.append(task)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()
            
            successful_translations = sum(1 for r in results if r is True)
            logger.info(f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ {successful_translations}/{len(all_chunks)} —á–∞–Ω–∫–æ–≤ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫")
            
            # 7. –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            logger.info("üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏...")
            output_project_dir = f"{self.config.source_project_path}_translated"
            
            # –í–æ—Å—Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
            merge_successful = 0
            validation_errors = []
            detailed_validation_results = {}
            
            for file_info in project_info['files_to_translate']:
                relative_path = file_info['path']
                output_file = os.path.join(output_project_dir, relative_path)
                if self.merger.merge_chunks(translated_chunks_dir, output_file, self.config.source_project_path):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    validation_result = self.validate_merged_file(file_info['full_path'], output_file)
                    detailed_validation_results[relative_path] = validation_result
                    
                    if validation_result['valid']:
                        merge_successful += 1
                    else:
                        validation_errors.append(f"{relative_path}: {validation_result['error']}")
                else:
                    detailed_validation_results[relative_path] = {
                        'valid': False,
                        'errors': ['Failed to merge chunks'],
                        'error': 'Failed to merge chunks'
                    }
            
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {merge_successful}/{len(project_info['files_to_translate'])} —Ñ–∞–π–ª–æ–≤")
            if validation_errors:
                logger.warning(f"–û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤: {len(validation_errors)}")
                for error in validation_errors[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                    logger.warning(f"  ‚Ä¢ {error}")
            
            # 8. –ö–æ–ø–∏—Ä—É–µ–º –Ω–µ—Ç—Ä–∞–Ω—Å–ª–∏—Ä—É–µ–º—ã–µ —Ñ–∞–π–ª—ã
            logger.info("üìÅ –ö–æ–ø–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã...")
            for root, dirs, files in os.walk(backup_dir):
                # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                dirs[:] = [d for d in dirs if d not in self.config.exclude_dirs]
                
                for file in files:
                    src_path = os.path.join(root, file)
                    rel_path = os.path.relpath(src_path, backup_dir)
                    dst_path = os.path.join(output_project_dir, rel_path)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª
                    _, ext = os.path.splitext(file)
                    if ext not in self.config.supported_extensions or file in self.config.exclude_files:
                        os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                        shutil.copy2(src_path, dst_path)
            
            logger.info(f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_project_dir}")
            
            # 9. –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
            self.create_translation_report(
                project_info, 
                output_project_dir, 
                successful_translations, 
                len(all_chunks),
                detailed_validation_results
            )
    
    async def translate_chunk_with_semaphore(self, semaphore: asyncio.Semaphore, chunk_path: str, output_dir: str):
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —á–∞–Ω–∫ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        async with semaphore:
            try:
                # –ß–∏—Ç–∞–µ–º —á–∞–Ω–∫
                with open(chunk_path, 'r', encoding='utf-8') as f:
                    chunk_data = json.load(f)
                
                # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                translated_content = await self.translator.translate_chunk(chunk_data['content'])
                
                # –û—á–∏—â–∞–µ–º –æ—Ç markdown –∫–æ–¥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                translated_content = self.clean_markdown_blocks(translated_content)
                
                # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                translated_content = self.validate_translation(chunk_data['content'], translated_content)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                chunk_data['content'] = translated_content
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —á–∞–Ω–∫
                output_path = os.path.join(output_dir, os.path.basename(chunk_path))
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(chunk_data, f, ensure_ascii=False, indent=2)
                
                return True
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞–Ω–∫–∞ {chunk_path}: {e}")
                return False
    
    def clean_markdown_blocks(self, content: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç markdown –∫–æ–¥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤"""
        import re
        
        # –£–±–∏—Ä–∞–µ–º markdown –∫–æ–¥–æ–≤—ã–µ –±–ª–æ–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        content = re.sub(r'^\s*```[\w]*\s*\n', '', content)
        content = re.sub(r'\n\s*```\s*$', '', content)
        
        return content
    
    def validate_translation(self, original_content: str, translated_content: str) -> str:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø–µ—Ä–µ–≤–æ–¥, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É"""
        original_lines = original_content.splitlines()
        translated_lines = translated_content.splitlines()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (–±–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ–¥—Ö–æ–¥)
        if len(original_lines) != len(translated_lines):
            line_diff = abs(len(original_lines) - len(translated_lines))
            logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {len(original_lines)} -> {len(translated_lines)}")
            # –¢–æ–ª—å–∫–æ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
            if line_diff > max(10, len(original_lines) * 0.15):  # –ë–æ–ª—å—à–µ 15% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–ª–∏ 10 —Å—Ç—Ä–æ–∫
                logger.error("–ü–µ—Ä–µ–≤–æ–¥ —Å–∏–ª—å–Ω–æ –∏—Å–∫–∞–∂–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª")
                return original_content
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É - –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å —Ç–∞–∫—É—é –∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if len(original_lines) > 0 and len(translated_lines) > 0:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
            orig_first = original_lines[0].strip()
            trans_first = translated_lines[0].strip()
            if orig_first and not orig_first.startswith('//') and not orig_first.startswith('#'):
                # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ –∫–æ–¥, –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è
                if len(orig_first.split()) > 0 and len(trans_first.split()) > 0:
                    orig_tokens = orig_first.replace('{', '').replace('}', '').replace('(', '').replace(')', '').split()
                    trans_tokens = trans_first.replace('{', '').replace('}', '').replace('(', '').replace(')', '').split()
                    if len(orig_tokens) > 0 and orig_tokens[0] != trans_tokens[0] if trans_tokens else True:
                        logger.warning("–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∞, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º")
                        translated_lines[0] = original_lines[0]
        
        return '\n'.join(translated_lines)
    
    def validate_merged_file(self, original_file_path: str, merged_file_path: str) -> Dict:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã
            with open(original_file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
                original_lines = original_content.splitlines()
            
            with open(merged_file_path, 'r', encoding='utf-8') as f:
                merged_content = f.read()
                merged_lines = merged_content.splitlines()
            
            errors = []
            
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            if len(original_lines) != len(merged_lines):
                errors.append(f'Line count mismatch: {len(original_lines)} vs {len(merged_lines)}')
            
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å Python (–µ—Å–ª–∏ —ç—Ç–æ Python —Ñ–∞–π–ª)
            if original_file_path.endswith('.py'):
                syntax_errors = self.validate_python_syntax(merged_content, merged_file_path)
                if syntax_errors:
                    errors.extend(syntax_errors)
            
            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–¥–∞
            original_structure = self.extract_structure(original_lines)
            merged_structure = self.extract_structure(merged_lines)
            
            if original_structure != merged_structure:
                structure_diff = self.find_structure_differences(original_lines, merged_lines)
                errors.append(f'Code structure changed: {structure_diff}')
            
            # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            chinese_validation = self.validate_chinese_characters(original_content, merged_content)
            if not chinese_validation['valid']:
                errors.extend(chinese_validation['errors'])
            
            # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—Ç—É–ø—ã
            indentation_errors = self.validate_indentation(original_lines, merged_lines)
            if indentation_errors:
                errors.extend(indentation_errors)
            
            return {
                'valid': len(errors) == 0,
                'errors': errors,
                'error': '; '.join(errors) if errors else None
            }
            
        except Exception as e:
            return {
                'valid': False,
                'errors': [f'Validation exception: {e}'],
                'error': f'Validation error: {e}'
            }
    
    def extract_structure(self, lines: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ–¥–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        structure_elements = []
        
        for line in lines:
            stripped = line.strip()
            if not stripped or stripped.startswith('#') or stripped.startswith('//'):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            structure_line = ''
            for char in stripped:
                if char in '{}()[];,=+-*/<>!&|':
                    structure_line += char
                elif char.isalnum() or char == '_':
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π —Å–∏–º–≤–æ–ª –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
                    if not structure_line or not structure_line[-1].isalnum():
                        structure_line += char
                elif char in ' \t':
                    if structure_line and not structure_line[-1] == ' ':
                        structure_line += ' '
            
            if structure_line:
                structure_elements.append(structure_line.strip())
        
        return '\n'.join(structure_elements)
    
    def validate_python_syntax(self, content: str, file_path: str) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å Python —Ñ–∞–π–ª–∞"""
        errors = []
        try:
            ast.parse(content)
        except SyntaxError as e:
            error_msg = f"Python syntax error at line {e.lineno}: {e.msg}"
            if hasattr(e, 'text') and e.text:
                error_msg += f" in '{e.text.strip()}'"
            errors.append(error_msg)
        except Exception as e:
            errors.append(f"Python parsing error: {e}")
        
        return errors
    
    def validate_chinese_characters(self, original: str, translated: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ custom_instructions)"""
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
        
        original_chinese = set(chinese_pattern.findall(original))
        translated_chinese = set(chinese_pattern.findall(translated))
        
        errors = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        preserve_chinese = 'PRESERVE_CHINESE_CHARACTERS' in self.config.custom_instructions
        translate_chinese = 'TRANSLATE_CHINESE_CHARACTERS' in self.config.custom_instructions
        
        if preserve_chinese:
            # –†–µ–∂–∏–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è
            lost_chars = original_chinese - translated_chinese
            if lost_chars:
                errors.append(f"Lost Chinese characters (should be preserved): {''.join(sorted(lost_chars))}")
            
            new_chars = translated_chinese - original_chinese
            if new_chars:
                errors.append(f"New Chinese characters appeared: {''.join(sorted(new_chars))}")
                
        elif translate_chinese:
            # –†–µ–∂–∏–º –ø–µ—Ä–µ–≤–æ–¥–∞: –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –¥–æ–ª–∂–Ω—ã –∏—Å—á–µ–∑–Ω—É—Ç—å (–ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã)
            remaining_chars = translated_chinese & original_chinese  # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            if remaining_chars:
                errors.append(f"Chinese characters not translated: {''.join(sorted(remaining_chars))}")
        
        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω —Ä–µ–∂–∏–º –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        return {
            'valid': len(errors) == 0,
            'errors': errors
        }
    
    def validate_indentation(self, original_lines: List[str], translated_lines: List[str]) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç—Å—Ç—É–ø–æ–≤"""
        errors = []
        
        if len(original_lines) != len(translated_lines):
            return errors  # –£–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ
        
        for i, (orig_line, trans_line) in enumerate(zip(original_lines, translated_lines), 1):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç—Å—Ç—É–ø—ã (–ø—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—ã –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏)
            orig_indent = len(orig_line) - len(orig_line.lstrip())
            trans_indent = len(trans_line) - len(trans_line.lstrip())
            
            if orig_indent != trans_indent:
                errors.append(f"Indentation mismatch at line {i}: {orig_indent} vs {trans_indent} spaces")
        
        return errors
    
    def find_structure_differences(self, original_lines: List[str], translated_lines: List[str]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ"""
        orig_brackets = self.count_brackets(original_lines)
        trans_brackets = self.count_brackets(translated_lines)
        
        differences = []
        for bracket_type, orig_count in orig_brackets.items():
            trans_count = trans_brackets.get(bracket_type, 0)
            if orig_count != trans_count:
                differences.append(f"{bracket_type}: {orig_count}->{trans_count}")
        
        return "; ".join(differences) if differences else "unknown structural change"
    
    def count_brackets(self, lines: List[str]) -> Dict[str, int]:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫–æ–±–æ–∫ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤"""
        brackets = defaultdict(int)
        
        for line in lines:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Å—Ç—Ä–æ–∫–∏
            in_string = False
            in_comment = False
            
            for i, char in enumerate(line):
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫
                if char in ['"', "'"] and (i == 0 or line[i-1] != '\\'):
                    in_string = not in_string
                    continue
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                if not in_string and char == '#':
                    in_comment = True
                    break
                
                if not in_string and not in_comment:
                    if char in '()[]{}':  
                        brackets[char] += 1
        
        return dict(brackets)
    
    def create_translation_report(self, project_info: Dict, output_dir: str, successful: int, total: int, validation_results: Dict = None):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø–µ—Ä–µ–≤–æ–¥–µ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–∞—Ö"""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation_summary = {
            'total_files_validated': 0,
            'files_with_errors': 0,
            'common_error_types': defaultdict(int),
            'files_by_status': {'valid': [], 'invalid': []}
        }
        
        if validation_results:
            validation_summary['total_files_validated'] = len(validation_results)
            
            for file_path, result in validation_results.items():
                if result['valid']:
                    validation_summary['files_by_status']['valid'].append(file_path)
                else:
                    validation_summary['files_by_status']['invalid'].append(file_path)
                    validation_summary['files_with_errors'] += 1
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –æ—à–∏–±–æ–∫
                    if 'errors' in result:
                        for error in result['errors']:
                            if 'Line count mismatch' in error:
                                validation_summary['common_error_types']['line_count_mismatch'] += 1
                            elif 'Python syntax error' in error:
                                validation_summary['common_error_types']['python_syntax_error'] += 1
                            elif 'Code structure changed' in error:
                                validation_summary['common_error_types']['structure_change'] += 1
                            elif 'Chinese characters' in error:
                                validation_summary['common_error_types']['chinese_characters'] += 1
                            elif 'Indentation mismatch' in error:
                                validation_summary['common_error_types']['indentation_error'] += 1
                            else:
                                validation_summary['common_error_types']['other'] += 1
        
        report = {
            "translation_summary": {
                "source_project": self.config.source_project_path,
                "target_language": self.config.target_language,
                "total_files": project_info['total_files'],
                "translatable_files": project_info['translatable_files'],
                "total_chunks": total,
                "successful_chunks": successful,
                "success_rate": f"{(successful/total)*100:.2f}%" if total > 0 else "0%",
                "file_types": project_info['file_types']
            },
            "validation_summary": validation_summary,
            "detailed_validation_results": validation_results or {},
            "config_used": {
                "llm_provider": self.config.llm_provider,
                "model_name": self.config.model_name,
                "chunk_size": self.config.chunk_size,
                "temperature": "0.0 (—Å–Ω–∏–∂–µ–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏)",
                "preserve_patterns": self.config.preserve_patterns,
                "max_concurrent_requests": self.config.max_concurrent_requests
            },
            "recommendations": self.generate_recommendations(validation_summary)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –æ—Ç—á–µ—Ç
        report_path = os.path.join(output_dir, "TRANSLATION_REPORT.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º —á–∏—Ç–∞–µ–º—ã–π Markdown –æ—Ç—á–µ—Ç
        md_report_path = os.path.join(output_dir, "TRANSLATION_REPORT.md")
        self.create_markdown_report(report, md_report_path)
        
        logger.info(f"üìä –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {report_path} –∏ {md_report_path}")
    
    def generate_recommendations(self, validation_summary: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        recommendations = []
        
        if validation_summary['files_with_errors'] == 0:
            recommendations.append("‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω –∏–¥–µ–∞–ª—å–Ω–æ! –í—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é.")
        else:
            error_types = validation_summary['common_error_types']
            
            if error_types.get('line_count_mismatch', 0) > 0:
                recommendations.append("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å—Ç—Ä–æ–∫. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ —Å—Ç—Ä–æ–≥–æ—Å—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.")
            
            if error_types.get('python_syntax_error', 0) > 0:
                recommendations.append("üêç –ù–∞–π–¥–µ–Ω—ã —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ Python. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ –Ω–∞ –≤—Å–µ—Ö .py —Ñ–∞–π–ª–∞—Ö.")
            
            if error_types.get('structure_change', 0) > 0:
                recommendations.append("üèóÔ∏è –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∫–æ–¥–∞. –í–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å –ø—Ä–æ–º–ø—Ç –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∫–æ–±–æ–∫ –∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤.")
            
            if error_types.get('chinese_characters', 0) > 0:
                recommendations.append("üá®üá≥ –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–∏—Ç–∞–π—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.")
            
            if error_types.get('indentation_error', 0) > 0:
                recommendations.append("üìè –û—à–∏–±–∫–∏ –æ—Ç—Å—Ç—É–ø–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª—ã Python –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.")
        
        return recommendations
    
    def create_markdown_report(self, report_data: Dict, output_path: str):
        """–°–æ–∑–¥–∞–µ—Ç —á–∏—Ç–∞–µ–º—ã–π Markdown –æ—Ç—á–µ—Ç"""
        md_content = f"""# üìä –û—Ç—á–µ—Ç –æ –ø–µ—Ä–µ–≤–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–∞

## üéØ –°–≤–æ–¥–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞

- **–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–µ–∫—Ç:** `{report_data['translation_summary']['source_project']}`
- **–¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫:** {report_data['translation_summary']['target_language']}
- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:** {report_data['translation_summary']['total_files']}
- **–§–∞–π–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:** {report_data['translation_summary']['translatable_files']}
- **–ß–∞–Ω–∫–æ–≤ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ:** {report_data['translation_summary']['successful_chunks']}/{report_data['translation_summary']['total_chunks']}
- **–£—Å–ø–µ—à–Ω–æ—Å—Ç—å:** {report_data['translation_summary']['success_rate']}

## üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏

- **–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ —Ñ–∞–π–ª–æ–≤:** {report_data['validation_summary']['total_files_validated']}
- **–§–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏:** {report_data['validation_summary']['files_with_errors']}
- **–§–∞–π–ª–æ–≤ –±–µ–∑ –æ—à–∏–±–æ–∫:** {len(report_data['validation_summary']['files_by_status']['valid'])}

### –¢–∏–ø—ã –æ—à–∏–±–æ–∫:

"""
        
        for error_type, count in report_data['validation_summary']['common_error_types'].items():
            error_name = {
                'line_count_mismatch': 'üìè –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫',
                'python_syntax_error': 'üêç –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ Python',
                'structure_change': 'üèóÔ∏è –ò–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–¥–∞',
                'chinese_characters': 'üá®üá≥ –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–∏—Ç–∞–π—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏',
                'indentation_error': 'üìê –û—à–∏–±–∫–∏ –æ—Ç—Å—Ç—É–ø–æ–≤',
                'other': '‚ùì –ü—Ä–æ—á–∏–µ –æ—à–∏–±–∫–∏'
            }.get(error_type, error_type)
            
            md_content += f"- {error_name}: {count}\n"
        
        md_content += f"""

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞

- **LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä:** {report_data['config_used']['llm_provider']}
- **–ú–æ–¥–µ–ª—å:** {report_data['config_used']['model_name']}
- **–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞:** {report_data['config_used']['chunk_size']} —Å—Ç—Ä–æ–∫
- **–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞:** {report_data['config_used']['temperature']}
- **–ú–∞–∫—Å–∏–º—É–º –∑–∞–ø—Ä–æ—Å–æ–≤:** {report_data['config_used']['max_concurrent_requests']}

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

"""
        
        for rec in report_data['recommendations']:
            md_content += f"- {rec}\n"
        
        if report_data['validation_summary']['files_with_errors'] > 0:
            md_content += f"""

## ‚ùå –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏

"""
            
            for file_path in report_data['validation_summary']['files_by_status']['invalid'][:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                validation_result = report_data['detailed_validation_results'].get(file_path, {})
                md_content += f"### `{file_path}`\n"
                if 'errors' in validation_result:
                    for error in validation_result['errors']:
                        md_content += f"- ‚ö†Ô∏è {error}\n"
                md_content += "\n"
        
        md_content += f"""

---
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞:** {time.strftime('%Y-%m-%d %H:%M:%S')}  
**–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫:** TransLLM Enhanced v2.0
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)


def load_static_config() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø–∞–ø–∫–∏ TransLLM"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_data = {}
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ config.json
    config_file = os.path.join(script_dir, "config.json")
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_file}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {}
    else:
        logger.error(f"‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_file}")
        return {}
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ user_rules.md
    user_rules_file = os.path.join(script_dir, "user_rules.md")
    if os.path.exists(user_rules_file):
        try:
            with open(user_rules_file, 'r', encoding='utf-8') as f:
                user_instructions = f.read().strip()
                config_data['user_instructions'] = user_instructions
            logger.info(f"üë§ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ {user_rules_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª: {e}")
    
    return config_data

def create_default_config_files(project_path: str):
    """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
    config_file = os.path.join(project_path, "translllm_config.json")
    sys_rules_file = os.path.join(project_path, "sys_rules.md")
    user_rules_file = os.path.join(project_path, "user_rules.md")
    
    # –°–æ–∑–¥–∞–µ–º config.json –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not os.path.exists(config_file):
        default_config = {
            "llm_provider": "groq",
            "model_name": "openai/gpt-oss-120b",
            "api_key": "YOUR_API_KEY_HERE",
            "target_language": "Russian",
            "source_language": "English",
            "chunk_size": 150,
            "temperature": 0.0,
            "max_concurrent_requests": 10,
            "supported_extensions": [".py", ".js", ".ts", ".jsx", ".tsx", ".html", ".css", ".java", ".cpp", ".c", ".h", ".cs", ".php", ".rb", ".go", ".rs", ".swift", ".kt"],
            "exclude_dirs": ["node_modules", "__pycache__", ".git", "venv", "env", "dist", "build"],
            "exclude_files": ["README.md", "LICENSE", "package.json", ".gitignore"]
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=2)
        logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {config_file}")
    
    # –°–æ–∑–¥–∞–µ–º sys_rules.md –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not os.path.exists(sys_rules_file):
        default_sys_rules = """# ü§ñ –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä–µ–≤–æ–¥–∞

## –ë–∞–∑–æ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- INPUT LINE COUNT = OUTPUT LINE COUNT (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)
- –°–æ—Ö—Ä–∞–Ω—è—Ç—å –¢–û–ß–ù–û: –æ—Ç—Å—Ç—É–ø—ã, –ø—Ä–æ–±–µ–ª—ã, —Å–∫–æ–±–∫–∏, —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π, –∑–∞–ø—è—Ç—ã–µ
- –°–æ—Ö—Ä–∞–Ω—è—Ç—å –¢–û–ß–ù–û: –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
- –ù–ï –¥–æ–±–∞–≤–ª—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, markdown
- –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π –∫–æ–¥

## –ß—Ç–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏: # –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, // –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, /* –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π */
- Docstrings: \"\"\"—Ç–µ–∫—Å—Ç\"\"\", '''—Ç–µ–∫—Å—Ç'''
- –°—Ç—Ä–æ–∫–∏ —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —è–∑—ã–∫–æ–º (–Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏)
- –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –æ—à–∏–±–∫–∏

## –ß—Ç–æ –ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
- –°–∏–Ω—Ç–∞–∫—Å–∏—Å –∫–æ–¥–∞, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
- –ò–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, —Ñ—É–Ω–∫—Ü–∏–π, –∫–ª–∞—Å—Å–æ–≤, –º–æ–¥—É–ª–µ–π
- Import statements, –ø—É—Ç–∏ —Ñ–∞–π–ª–æ–≤, URLs
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –∫–ª—é—á–∏
- –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è, SQL, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"""
        
        with open(sys_rules_file, 'w', encoding='utf-8') as f:
            f.write(default_sys_rules)
        logger.info(f"üìú –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª: {sys_rules_file}")
    
    # –°–æ–∑–¥–∞–µ–º user_rules.md –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not os.path.exists(user_rules_file):
        default_user_rules = """# üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä–µ–≤–æ–¥–∞

## –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞

### –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –∫–∏—Ç–∞–π—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏:
<!-- –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –Ω—É–∂–Ω—ã–π —Ä–µ–∂–∏–º -->

<!-- –†–µ–∂–∏–º 1: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤)
PRESERVE_CHINESE_CHARACTERS
–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –í–°–ï –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã —Ç–æ—á–Ω–æ: Áî≤‰πô‰∏ô‰∏ÅÊàäÂ∑±Â∫öËæõÂ£¨Áô∏Â≠ê‰∏ëÂØÖÂçØËæ∞Â∑≥ÂçàÊú™Áî≥ÈÖâÊàå‰∫•Êú®ÁÅ´ÂúüÈáëÊ∞¥
–≠—Ç–æ –≤–∞–∂–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ò–ö–û–ì–î–ê –Ω–µ–ª—å–∑—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å.
-->

<!-- –†–µ–∂–∏–º 2: –ü–µ—Ä–µ–≤–æ–¥ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤)
TRANSLATE_CHINESE_CHARACTERS
–ü–µ—Ä–µ–≤–æ–¥–∏—Ç—å –í–°–ï –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫.
–ö–∏—Ç–∞–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç.
-->

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:
- –î–æ–±–∞–≤—å—Ç–µ –∑–¥–µ—Å—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
- –û—Å–æ–±—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
- –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
"""
        
        with open(user_rules_file, 'w', encoding='utf-8') as f:
            f.write(default_user_rules)
        logger.info(f"üë§ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª: {user_rules_file}")

async def main():
    parser = argparse.ArgumentParser(description="Universal project translator using LLM - Enhanced v2.1")
    parser.add_argument("project_path", help="Path to project for translation")
    parser.add_argument("--target-lang", help="Target language (overrides config)")
    parser.add_argument("--source-lang", help="Source language (overrides config)")
    parser.add_argument("--llm-provider", choices=["groq", "openai", "anthropic"], help="LLM provider (overrides config)")
    parser.add_argument("--api-key", help="API key for LLM provider (overrides config)")
    parser.add_argument("--model", help="Model name to use (overrides config)")
    parser.add_argument("--chunk-size", type=int, help="Chunk size in lines (overrides config)")
    parser.add_argument("--max-concurrent", type=int, help="Max concurrent requests (overrides config)")
    parser.add_argument("--user-rules", help="Custom user rules for this project (overrides template)")
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø–∞–ø–∫–∏ TransLLM
    static_config = load_static_config()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if not static_config:
        logger.error("‚ùå Static configuration files not found!")
        logger.info("üí° Make sure config.json exists in TransLLM directory with proper API key")
        return
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —Ñ–∞–π–ª–æ–≤—ã–µ)
    target_language = args.target_lang or static_config.get("target_language", "Russian")
    source_language = args.source_lang or static_config.get("source_language", "English")
    llm_provider = args.llm_provider or static_config.get("llm_provider", "groq")
    api_key = args.api_key or static_config.get("api_key")
    chunk_size = args.chunk_size or static_config.get("chunk_size", 100)
    max_concurrent = args.max_concurrent or static_config.get("max_concurrent_requests", 10)
    model_name = args.model or static_config.get("model_name")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á
    if not api_key or api_key == "YOUR_API_KEY_HERE":
        logger.error("‚ùå API key not set!")
        logger.info("üí° Set API key in config.json or pass via --api-key")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
    if not model_name:
        model_defaults = {
            "groq": "llama-3.3-70b-versatile",
            "openai": "gpt-4",
            "anthropic": "claude-3-sonnet-20240229"
        }
        model_name = model_defaults[llm_provider]
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    user_instructions = args.user_rules or static_config.get('user_instructions', '')
    
    config = TranslationConfig(
        source_project_path=args.project_path,
        target_language=target_language,
        source_language=source_language,
        chunk_size=chunk_size,
        custom_instructions=user_instructions,
        llm_provider=llm_provider,
        api_key=api_key,
        model_name=model_name,
        max_concurrent_requests=max_concurrent,
        supported_extensions=static_config.get("supported_extensions"),
        exclude_dirs=static_config.get("exclude_dirs"),
        exclude_files=static_config.get("exclude_files")
    )
    
    logger.info("üöÄ Configuration loaded:")
    logger.info(f"   ü§ñ LLM: {llm_provider}/{model_name}")
    logger.info(f"   üåê Languages: {source_language} ‚Üí {target_language}")
    logger.info(f"   üì¶ Chunks: {chunk_size} lines, {max_concurrent} concurrent")
    logger.info(f"   üìÅ Project: {args.project_path}")
    
    translator = ProjectTranslator(config)
    await translator.translate_project()


if __name__ == "__main__":
    asyncio.run(main())
